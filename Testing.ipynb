{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tasks for BBS over: ['2023-06-01', '2024-05-31']\n",
      "PPFD_IN_1_1_1 missing, outputting NaNs\n",
      "See output: C:/highfreq/BBS/auxilaryData/BBS_biometData_202306010000_202405310000.csv\n",
      "See output: C:/highfreq/BBS/auxilaryData/BBS_dynamicMetadata_202306010000_202405310000.csv\n",
      "All tasks completed successfully\n",
      "{'biometData': 'C:/highfreq/BBS/auxilaryData/BBS_biometData_202306010000_202405310000.csv', 'dynamicMetadata': 'C:/highfreq/BBS/auxilaryData/BBS_dynamicMetadata_202306010000_202405310000.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Create biomet and dynamicMetadata.csv files\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "BiometNet = 'C:/Biomet.net/Python/'\n",
    "\n",
    "# UBC Micromet users can \n",
    "if sys.path[0]!=BiometNet:\n",
    "    sys.path.insert(0,BiometNet)\n",
    "\n",
    "wd = [p for p in sys.path if p != BiometNet][0]\n",
    "\n",
    "import csvFromBinary as cfb\n",
    "importlib.reload(cfb)\n",
    "\n",
    "siteID = 'BBS'\n",
    "dateRange = ['2023-06-01','2024-05-31']\n",
    "tasks=os.path.abspath(wd+'/config_files/EP_auxillary_data_defs.yml')\n",
    "auxilaryDpaths=cfb.makeCSV(siteID,dateRange,tasks,\n",
    "                           outputPath=f'C:/highfreq/{siteID}/auxilaryData',stage='Second')\n",
    "print(auxilaryDpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching C:/Campbellsci/CardConvert/20240510\n",
      "[████████████████████████████████████████████████████████████] 1061/1061\n",
      "\n",
      "Files Search Complete, time elapsed:  10.486396074295044\n"
     ]
    }
   ],
   "source": [
    "import eddyProAPI\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import time\n",
    "importlib.reload(eddyProAPI)\n",
    "\n",
    "# Basic call for .ghg files\n",
    "\n",
    "siteID = 'BBS'\n",
    "dateRange=[(pd.Timestamp.now()-pd.Timedelta(days=365)).strftime('%Y%m%d'),pd.Timestamp.now().strftime('%Y%m%d')],\n",
    "\n",
    "\n",
    "if siteID == 'BB':\n",
    "    pp = eddyProAPI.preProcessing('BB',dateRange=dateRange,Testing=5)\n",
    "    pp.searchRawDir()\n",
    "elif siteID == 'BBS':\n",
    "                \n",
    "    # # Basic call for .dat files\n",
    "    metaDataTemplate='C:/highfreq/BBS/TOA5_BBS.FLUX_2023_06_14_1500.metadata'\n",
    "    pp = eddyProAPI.preProcessing('BBS',dateRange=dateRange,fileType='dat',metaDataTemplate=metaDataTemplate,\n",
    "                                # debug=True,\n",
    "                                reset=True,\n",
    "    )\n",
    "    pp.searchRawDir(\n",
    "                copyFrom = \"C:/Campbellsci/CardConvert/20240510\",\n",
    "                searchTag = \"BBS.FLUX\",\n",
    "                timeShift=30)\n",
    "# pp.readFiles()\n",
    "# pp.groupAndFilter()\n",
    "# with shutil 186.0170259475708 seconds\n",
    "# via subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = ['copy', '\"C:\\\\Campbellsci\\\\CardConvert\\\\20240510\\\\TOA5_BBS.FLUX_2024_04_18_1251.dat\"', '\"C:\\\\highfreq\\\\BBS\\\\raw\\\\2024\\\\04\\\\TOA5_BBS.FLUX_2024_04_18_1321.dat\"']\n",
    "subprocess.call(cmd,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_prototype</th>\n",
       "      <th>Filter Flags</th>\n",
       "      <th>groupID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-04-18 13:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1321.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 13:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1330.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 14:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1400.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 14:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1430.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 15:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1500.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 15:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1530.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 16:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1600.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 16:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1630.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 17:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1700.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 17:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1730.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 18:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1800.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 18:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1830.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 19:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1900.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 19:30:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_1930.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-18 20:00:00</th>\n",
       "      <td>C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_2024_04_18_2000.dat</td>\n",
       "      <td>group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat</td>\n",
       "      <td>~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                source  \\\n",
       "TIMESTAMP                                                                \n",
       "2024-04-18 13:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 13:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 14:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 14:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 15:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 15:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 16:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 16:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 17:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 17:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 18:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 18:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 19:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 19:30:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "2024-04-18 20:00:00  C:\\Campbellsci\\CardConvert\\20240510/TOA5_BBS.F...   \n",
       "\n",
       "                                                      filename  \\\n",
       "TIMESTAMP                                                        \n",
       "2024-04-18 13:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1321.dat   \n",
       "2024-04-18 13:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1330.dat   \n",
       "2024-04-18 14:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1400.dat   \n",
       "2024-04-18 14:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1430.dat   \n",
       "2024-04-18 15:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1500.dat   \n",
       "2024-04-18 15:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1530.dat   \n",
       "2024-04-18 16:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1600.dat   \n",
       "2024-04-18 16:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1630.dat   \n",
       "2024-04-18 17:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1700.dat   \n",
       "2024-04-18 17:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1730.dat   \n",
       "2024-04-18 18:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1800.dat   \n",
       "2024-04-18 18:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1830.dat   \n",
       "2024-04-18 19:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_1900.dat   \n",
       "2024-04-18 19:30:00  group_1_TOA5_BBS.FLUX_2024_04_18_1930.dat   \n",
       "2024-04-18 20:00:00  group_1_TOA5_BBS.FLUX_2024_04_18_2000.dat   \n",
       "\n",
       "                                                file_prototype Filter Flags  \\\n",
       "TIMESTAMP                                                                     \n",
       "2024-04-18 13:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 13:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 14:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 14:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 15:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 15:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 16:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 16:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 17:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 17:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 18:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 18:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 19:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 19:30:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "2024-04-18 20:00:00  group_1_TOA5_BBS.FLUX_yyyy_mm_dd_HHMM.dat            ~   \n",
       "\n",
       "                     groupID  \n",
       "TIMESTAMP                     \n",
       "2024-04-18 13:00:00        1  \n",
       "2024-04-18 13:30:00        1  \n",
       "2024-04-18 14:00:00        1  \n",
       "2024-04-18 14:30:00        1  \n",
       "2024-04-18 15:00:00        1  \n",
       "2024-04-18 15:30:00        1  \n",
       "2024-04-18 16:00:00        1  \n",
       "2024-04-18 16:30:00        1  \n",
       "2024-04-18 17:00:00        1  \n",
       "2024-04-18 17:30:00        1  \n",
       "2024-04-18 18:00:00        1  \n",
       "2024-04-18 18:30:00        1  \n",
       "2024-04-18 19:00:00        1  \n",
       "2024-04-18 19:30:00        1  \n",
       "2024-04-18 20:00:00        1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.fileInventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating C:/highfreq//BBS/metadata/group_1.eddypro\n"
     ]
    }
   ],
   "source": [
    "# Cospectral correction options\n",
    "hf_meth = {\n",
    "    'Moncrieff et al. (1997)':1,\n",
    "    'Horst (1997)':2,\n",
    "    'Ibrom et al. (2007)':3,\n",
    "    'Fratini et al. (2012)':4, # Recommended , when sufficient data (>1 month) are available\n",
    "    'Massman (2000, 2001)':5\n",
    "}\n",
    "\n",
    "userDefinedEddyProSettings = {\n",
    "    'Project':{\n",
    "        'hf_meth':hf_meth['Fratini et al. (2012)'],\n",
    "        },\n",
    "    'RawProcess_Settings':{\n",
    "        'v_offset':0.08,\n",
    "        },\n",
    "    }\n",
    "\n",
    "import eddyProAPI\n",
    "import importlib\n",
    "importlib.reload(eddyProAPI)\n",
    "\n",
    "siteID = 'BBS'\n",
    "\n",
    "eP = eddyProAPI.runEP(siteID,userDefinedEddyProSettings=userDefinedEddyProSettings,**auxilaryDpaths)\n",
    "# eP.configurationGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'csat3_1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if 'Project' in userDefinedEddyProSettings.keys():\n",
    "#     print('P')\n",
    "eP.groupMetaData['Instruments']['instr_1_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To DO: Add Output ID on merge so we can trace down specific thread/log file for a given run\n",
    "## Same timestamp can have different outputs depending on batch it was run in\n",
    "## Usually minor differences, but can be larger in select circumstances\n",
    "\n",
    "import setupEP as eP\n",
    "import importlib\n",
    "import time\n",
    "importlib.reload(eP)\n",
    "\n",
    "########## Note - incomplete ghg or biomet files can crash program (preprocessing procedures should help prevent that by re-naming incomplete files)\n",
    "\n",
    "T1 = time.time()\n",
    "mR = eP.makeRun('ep_Templates/DefaultSettings.eddypro','BBS',Processes=6,priority = 'high priority')\n",
    "mR.runDates(['2023-01-01 00:00','2024-03-31 23:59'])\n",
    "\n",
    "# mR = eP.makeRun('ep_Templates/LabStandard_Advanced.eddypro','BB',Processes=4,priority = 'high priority')\n",
    "# mR.updateTemplate({'FluxCorrection_SpectralAnalysis_General':\n",
    "#                     {'sa_mode':4}\n",
    "#                 })\n",
    "# mR.runDates(['2015-01-01 00:00','2023-12-31 23:59'])\n",
    "\n",
    "T2 = time.time()\n",
    "print('\\n')\n",
    "print('Runtime: ',(T2-T1)/60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read .ghg metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preProcessing\n",
    "import importlib\n",
    "import time\n",
    "importlib.reload(preProcessing)\n",
    "T1 = time.time()\n",
    "\n",
    "# Need one or more metadata templates if working with .dat files\n",
    "template = ['Y:/BBS/TOA5_BBS.FLUX_2023_06_14_1500.metadata']\n",
    "\n",
    "copyFrom = \"X:/BBS/EC_Station/2023/20230616\"\n",
    "copyTag = \"BBS.FLUX\"\n",
    "pr = preProcessing.read_ALL('BBS',\n",
    "                            fileType='dat',\n",
    "                            reset=1,\n",
    "                            metadataTemplate=template,\n",
    "                            copyFrom=copyFrom,\n",
    "                            copyTag=copyTag,\n",
    "                            timeShift=30)\n",
    "# pr.find_files(2023,6,processes=4)\n",
    "\n",
    "# updates = 'Y:/BB/Metadata_Updates.csv'\n",
    "# pr = preProcessing.read_ALL('BB',fileType='ghg',metadataUpdates=updates,reset=1)\n",
    "# # If updates/corrections must be made to metadata (e.g., undocumented orientation changes) a .csv files of updates can be provided\n",
    "\n",
    "# for y in range(2015,2024):\n",
    "#     for m in range(1,13):\n",
    "#         pr.find_files(y,m)\n",
    "\n",
    "# T2 = time.time()\n",
    "\n",
    "# print('\\n')\n",
    "# print('Runtime: ',(T2-T1)/60,' minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Implement pre-processing procedures to exclude data by conditions (e.g., low flow rate)\n",
    "pr.ini['dat']\n",
    "# import numpy as np\n",
    "# pr.dataRecords['flow_rate_7200']\n",
    "# pr.dataRecords.loc[pr.dataRecords['flow_rate_7200'].abs()>1e2,'flow_rate_7200']=np.nan\n",
    "# pr.dataRecords['flow_rate_7200'].plot()\n",
    "# # pr.dataRecords['col_air_t'].plot()#['altitude'].describe()\n",
    "# 317/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.licor.com/env/support/EddyPro/topics/low-pass-filtering.html\n",
    "\n",
    "# bin_sp_avail=[0,1]\n",
    "# full_sp_avail=[0,1]\n",
    "# sa_bin_spectra='Path'\n",
    "# sa_full_spectra = 'Path'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Runtime\n",
    "\n",
    "### Reading & Writing all to Y: drive:\n",
    "\n",
    "1 Month of BB data (20 hz)\n",
    "\n",
    "**Preprocessing**: 3.91 minutes (8 cores)\n",
    "**Processing**: 56.92 minutes (6 cores)\n",
    "\n",
    "Total: **60.83** minutes\n",
    "\n",
    "### Reading & Preprocessing on Y: drive then writing EddyPro results locally:\n",
    "\n",
    "1 Month of BB data (20 hz)\n",
    "\n",
    "**Preprocessing**: 3.91 minutes (8 cores)\n",
    "**Processing**: 55.74 minutes (6 cores)\n",
    "\n",
    "Total: **59.65** minutes\n",
    "\n",
    "### Copying data to C then writing EddyPro results locally:\n",
    "\n",
    "1 Month of BB data (20 hz)\n",
    "\n",
    "**Preprocessing**: 6.30 minutes (1 core copy > 8 core preprocessing)\n",
    "\n",
    "**Processing**: 54.33 minutes (6 cores)\n",
    "\n",
    "Total: **60.3** minutes\n",
    "\n",
    "* Paralellizing data copy could shave enought time to get marginal benfit, but doesn't look to matter much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(mR.all_outputs['fulloutput'],skiprows=[0,2],na_values=-9999,parse_dates={'TIMESTAMP':['date','time']},index_col='TIMESTAMP')\n",
    "# Add routine to handle duplicates (or find rood cause of them )\n",
    "# Occuring on boundary between batches I assume as there are ~ 2x as many duplicates as batches\n",
    "df = df.loc[df.index.duplicated()==False].copy()\n",
    "\n",
    "bm = pd.read_csv(mR.all_outputs['biomet'],skiprows=[1],na_values=-9999,parse_dates={'TIMESTAMP':['date','time']},index_col='TIMESTAMP')\n",
    "bm = bm.loc[bm.index.duplicated()==False].copy()\n",
    "\n",
    "\n",
    "df = pd.concat([df,bm[bm.columns[~bm.columns.isin(df.columns)]]],axis=1)\n",
    "df.loc[df['qc_co2_flux']>0,'co2_flux']=np.nan\n",
    "df.loc[df['u*']<0.15,'co2_flux']=np.nan\n",
    "\n",
    "plt.scatter(df['Rg_1_1_1'],df['co2_flux'])\n",
    "\n",
    "# df.loc[df['co2_flux']>20,['wind_speed','u*','wind_dir','Ta_1_1_1','H']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csvFromBinary as cfb\n",
    "import importlib\n",
    "importlib.reload(cfb)\n",
    "\n",
    "\n",
    "# BM\n",
    "cfb.makeBiomet('BBS',['2023-06-01 00:00','2024-05-31 23:59'],'C:/highfreq/BBS/metadata',type='dynamicMetadata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = ''\n",
    "# type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "# bm.duplicated().sum()\n",
    "# bm.columns[bm.columns.duplicated()]\n",
    "# T1 = df.iloc[0:10].copy()\n",
    "# T2 = bm[bm.columns[~bm.columns.isin(df.columns)]].iloc[0:10].copy()\n",
    "\n",
    "\n",
    "# T3 = pd.concat([T1,T2],axis=1)\n",
    "# T3\n",
    "rep = df[df.index.duplicated(keep=False)].sort_index().copy()#.duplicated()#['co2_flux']\n",
    "# print(rep.shape)\n",
    "for c in rep.columns:\n",
    "    s = rep.loc[rep[c].duplicated(keep=False)==False,c]\n",
    "\n",
    "    if s.shape[0]>1:\n",
    "        # print(c)\n",
    "        # print(s.shape[0])\n",
    "        print(rep[['filename',c]].groupby('filename').diff().dropna().min())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "monc = 'Y:/BB/Spectral_Comp/Moncrieff/'+'eddypro_BB_20230601_20230630_full_output_2024-03-15T153700_adv.csv'\n",
    "frat = 'Y:/BB/Spectral_Comp/Fratini/'+'eddypro_Spectral_Correction_fratini_full_output_2024-03-19T000447_adv.csv'\n",
    "df_fratini = pd.read_csv(frat,skiprows=[0,2],na_values=-9999,parse_dates={'TIMESTAMP':['date','time']},index_col='TIMESTAMP')\n",
    "df_moncrieff = pd.read_csv(monc,skiprows=[0,2],na_values=-9999,parse_dates={'TIMESTAMP':['date','time']},index_col='TIMESTAMP')\n",
    "\n",
    "def Filt(df):\n",
    "    df['filt'] = 1\n",
    "    df.loc[df['u*']<.15,'filt']=np.nan\n",
    "    df.loc[df['qc_co2_flux']>0,'filt']=np.nan\n",
    "    df.loc[df['qc_LE']>0,'filt']=np.nan\n",
    "    return(df)\n",
    "\n",
    "df_fratini = Filt(df_fratini)\n",
    "df_moncrieff = Filt(df_moncrieff)\n",
    "\n",
    "\n",
    "# df_fratini=df_fratini.loc[df_fratini.index.month==6].copy()\n",
    "\n",
    "cols = ['filt','u*','co2_flux','qc_co2_flux','LE','qc_LE','H','qc_H','ch4_flux','qc_ch4_flux']\n",
    "frat_cols = {key: key+'_fratini' for key in cols}\n",
    "monc_cols = {key: key+'_moncrieff' for key in cols}\n",
    "\n",
    "df_fratini = df_fratini[cols].rename(columns=frat_cols)\n",
    "df_moncrieff = df_moncrieff[cols].rename(columns=monc_cols)\n",
    "\n",
    "df = df_fratini.join(df_moncrieff)\n",
    "\n",
    "df['filt']=df['filt_fratini']*df['filt_moncrieff']\n",
    "\n",
    "for col in df.columns:\n",
    "    if col !='filt':\n",
    "        df[col]=df[col]*df['filt']\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "\n",
    "ax[0,0].scatter(df['co2_flux_fratini'],df['co2_flux_moncrieff'],label=f\"r2 = {(df[['co2_flux_fratini','co2_flux_moncrieff']].corr()**2)['co2_flux_moncrieff'].round(3).values[0]}\")\n",
    "ax[0,0].set_xlabel('Fratini et al. (2012)')\n",
    "ax[0,0].set_ylabel('Moncrieff et al. (1997)')\n",
    "ax[0,0].set_title('CO2 Flux (umol m-2 s-1)')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].grid()\n",
    "\n",
    "df[['co2_flux_fratini','co2_flux_moncrieff']].boxplot(ax=ax[0,1])\n",
    "ax[0,1].scatter([1,2],df[['co2_flux_fratini','co2_flux_moncrieff']].mean().values,marker='*',s=40,color='k')\n",
    "ax[0,1].set_xticklabels(['Fratini et al. (2012)','Moncrieff et al. (1997)'])\n",
    "ax[0,1].set_title('CO2 Flux (umol m-2 s-1)')\n",
    "\n",
    "\n",
    "ax[1,0].scatter(df['LE_fratini'],df['LE_moncrieff'],label=f\"r2 = {(df[['LE_fratini','LE_moncrieff']].corr()**2)['LE_moncrieff'].round(3).values[0]}\")\n",
    "ax[1,0].set_xlabel('Fratini et al. (2012)')\n",
    "ax[1,0].set_ylabel('Moncrieff et al. (1997)')\n",
    "ax[1,0].set_title('Latent Heat Flux (W m-2)')\n",
    "ax[1,0].legend()\n",
    "ax[1,0].grid()\n",
    "\n",
    "df[['LE_fratini','LE_moncrieff']].boxplot(ax=ax[1,1])\n",
    "ax[1,1].scatter([1,2],df[['LE_fratini','LE_moncrieff']].mean().values,marker='*',s=40,color='k')\n",
    "ax[1,1].set_xticklabels(['Fratini et al. (2012)','Moncrieff et al. (1997)'])\n",
    "ax[1,1].set_title('Latent Heat Flux (W m-2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "df[['LE_fratini','LE_moncrieff','co2_flux_fratini','co2_flux_moncrieff','ch4_flux_fratini','ch4_flux_moncrieff',]].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speedtests\n",
    "\n",
    "All times in minutes\n",
    "\n",
    "### 4 threads (8 timesteps each)\n",
    "\n",
    "Normal priority: 2.05\n",
    "High priority: 1.91\n",
    "\n",
    "### 4 threads (48 timesteps each x 12 days)\n",
    "\n",
    "High priority: 29\n",
    "\n",
    "### Eddypro GUI (1 run [12 days, 576 timesteps])\n",
    "\n",
    "High priority: 90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef0cd03416062db1397a6f4c064b2fd8fe41d0681561742315105bdf1dc915d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
